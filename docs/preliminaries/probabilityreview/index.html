<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Probability Review</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/preliminaries/probabilityreview/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Probability review</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>We will go through a review of probability concepts over here, all of review the material have been adapted from <a href="http://cs229.stanford.edu/section/cs229-prob.pdf">CS229 Probability Notes</a>.</p>

<h4 id="1-elements-of-probability">1. Elements of probability</h4>
<p>In order to define a probability on a set we need a few basic elements,</p>
<ul>
  <li><strong>Sample space</strong> Ω: The set of all the outcomes of a random experiment. Here, each outcome ω ∈ Ω can be thought of as a complete description of the state of the real world at the end of the experiment.</li>
  <li><strong>Set of events (or event space) F</strong>: A set whose elements A ∈ F (called events) are subsets of Ω (i.e., A ⊆ Ω is a collection of possible outcomes of an experiment)</li>
  <li><strong>Probability measure</strong>: A function P : F → R that satisfies the following properties,
    <ul>
      <li>P(A) ≥ 0, for all A ∈ F</li>
      <li>P(Ω) = 1</li>
      <li>If $A_1$, $A_2$, . . . are disjoint events $(i.e., A_i ∩ A_j = ∅  \text{ whenever }  i \neq j)$, then $P(∪_iA_i) = \sum_i P(A_i)$</li>
    </ul>
  </li>
</ul>

<p>These three properties are called the <strong>Axioms of Probability</strong>.</p>

<p><strong>Example</strong>: Consider the event of tossing a six-sided die. The sample space is Ω = {1, 2, 3, 4, 5, 6}. We can define different event spaces on this sample space. For example, the simplest event space is the trivial event space F = {∅, Ω}. Another event space is the set of all subsets of Ω. For the first event space, the unique probability measure satisfying the requirements above is given by P(∅) = 0, P(Ω) = 1. For the second event space, one valid probability measure is to assign the probability of each set in the event space to be $\frac{i}{6}$ where $i$ is the number of elements of that set; for example, $P({1, 2, 3, 4}) = \frac{4}{6}$ and $P({1, 2, 3}) = \frac{3}{6}$.</p>

<h4 id="properties">Properties:</h4>
<ul>
  <li>If A ⊆ B ⇒ P(A) ≤ P(B).</li>
  <li>$P(A \cap B) \leq min(P(A), P(B))$.</li>
  <li>(Union Bound) P(A ∪ B) ≤ P(A) + P(B).</li>
  <li>P(Ω \ A) = 1 − P(A).</li>
  <li>(Law of Total Probability) If $A_1, . . . , A_k$ are a set of disjoint events such that 
$\bigcup^k_{i=1} A_i = \Omega$ then $\sum^k_{i=1} P(A_k) = 1.$</li>
</ul>

<h4 id="11-conditional-probability-and-independence">1.1 Conditional probability and independence</h4>

<p>Let B be an event with non-zero probability. The conditional probability of any event A given B is defined as
$ P(A \mid B) = \frac {P(A \cap B)}{P(B)}$
In other words, $P(A \mid B)$ is the probability measure of the event A after observing the occurrence of
event B. Two events are called independent if and only if $P(A \cap B) = P(A)P(B)$ (or equivalently,
$P(A \mid B) = P(A)$). Therefore, independence is equivalent to saying that observing B does not have
any effect on the probability of A.</p>

<h4 id="2-random-variables">2. Random variables</h4>

<p>Consider an experiment in which we flip 10 coins, and we want to know the number of coins that
come up heads. Here, the elements of the sample space Ω are 10-length sequences of heads and
tails. For example, we might have $w_0 = \langle H, H, T, H, T, H, H, T, T, T \rangle \in Omega$. However, in practice, we usually do not care about the probability of obtaining any particular sequence of heads and tails. Instead we usually care about real-valued functions of outcomes, such as the number of heads that
appear among our 10 tosses, or the length of the longest run of tails. These functions, under some
technical conditions, are known as <strong>random variables</strong>.</p>

<p>More formally, a random variable $X$ is a function $X : \Omega → I!R$. Typically, we will denote random
variables using upper case letters $X(\omega)$ or more simply $X$ (where the dependence on the random
outcome $\omega$ is implied). We will denote the value that a random variable may take on using lower
case letters $x$.</p>

<p><strong>Example</strong>: In our experiment above, suppose that $X(\omega)$ is the number of heads which occur in the
sequence of tosses ω. Given that only 10 coins are tossed, $X(\omega)$ can take only a finite number of
values, so it is known as a discrete random variable. Here, the probability of the set associated
with a random variable X taking on some specific value $k$ is
    $P(X = k) := P({\omega : X(\omega) = k})$.</p>

<p><strong>Example</strong>: Suppose that $X(\omega)$ is a random variable indicating the amount of time it takes for a
radioactive particle to decay. In this case, $X(\omega)$ takes on a infinite number of possible values, so it is
called a continuous random variable. We denote the probability that X takes on a value between
two real constants a and b (where a &lt; b) as $P(a \leq X \leq b) := P({\omega : a \leq X(\omega) \leq b})$.</p>

<h4 id="21-cumulative-distribution-functions">2.1 Cumulative distribution functions</h4>

<p>In order to specify the probability measures used when dealing with random variables, it is often convenient to specify alternative functions (CDFs, PDFs, and PMFs) from which the probability measure governing an experiment immediately follows. In this section and the next two sections, we describe each of these types of functions in turn. A cumulative distribution function (CDF) is a function $F_X : I!R → [0, 1]$ which specifies a probability measure as, 
\begin{equation}
F_X(x) = P(X \leq x)
\end{equation}
By using this function one can calculate the probability of any event.</p>

<h5 id="properties-1">Properties:</h5>
<!--Figure 1: A cumulative distribution function (CDF).-->
<ul>
  <li>$0 ≤ F_X(x) ≤ 1$.</li>
  <li>$lim_{x→-\infty} F_X(x) = 0$.</li>
  <li>$lim_{x→\infty} F_X(x) = 1$.</li>
  <li>$x \leq y ⇒ F_X(x) \leq F_X(y)$.</li>
</ul>

<h4 id="22-probability-mass-functions">2.2 Probability mass functions</h4>
<p>When a random variable X takes on a finite set of possible values (i.e., X is a discrete random
variable), a simpler way to represent the probability measure associated with a random variable is
to directly specify the probability of each value that the random variable can assume. In particular,
a probability mass function (PMF) is a function $pX : \Omega → I!R$ such that 
$p_X(x) = P(X = x)$.</p>

<p>In the case of discrete random variable, we use the notation V al(X) for the set of possible values that the random variable X may assume. For example, if X(ω) is a random variable indicating the number of heads out of ten tosses of coin, then V al(X) = {0, 1, 2, . . . , 10}.</p>

<h5 id="properties-2">Properties:</h5>
<ul>
  <li>$0 ≤ p_X(x) ≤ 1$.</li>
  <li>$\sum_{x∈Val(X)} p_X(x) = 1$.</li>
  <li>$\sum{x∈A} p_X(x) = P(X \in A)$.</li>
</ul>

<h4 id="23-probability-density-functions">2.3 Probability density functions</h4>
<p>For some continuous random variables, the cumulative distribution function FX(x) is differentiable everywhere. In these cases, we define the Probability Density Function or PDF as the derivative of the CDF, i.e.,</p>

<p>$f_X(x)  = \frac{dF_X(x)}{dx}$.</p>

<p>Note here, that the PDF for a continuous random variable may not always exist (i.e., if FX(x) is not differentiable everywhere).</p>

<p>According to the properties of differentiation, for very small ∆x,</p>

<p>$P(x \leq X \leq x + \delta x) ≈ f_X(x)\delta x$.</p>

<p>Both CDFs and PDFs (when they exist!) can be used for calculating the probabilities of different events. But it should be emphasized that the value of PDF at any given point $x$ is not the probability of that event, i.e., $f_X(x) \neq P(X = x)$. For example, $f_X(x)$ can take on values larger than one (but the integral of fX(x) over any subset of R will be at most one).</p>

<h5 id="properties-3">Properties:</h5>
<ul>
  <li>$f_X(x) \geq 0$</li>
  <li>$\int^{\infty}_{-\infty} f_X(x) = 1$.</li>
  <li>$\int_{x \in A} f_X(x) dx = P(X \in A)$.</li>
</ul>

<h4 id="24-expectation">2.4 Expectation</h4>

<p>Suppose that $X$ is a discrete random variable with <strong>PMF</strong> $p_X(x)$ and $g : I!R \rightarrow I!R$ is an arbitrary function. In this case, $g(X)$ can be considered a random variable, and we define the expectation or expected value of $g(X)$ as:</p>

<p>$E[g(X)] = \sum_{x∈Val(X)} g(x)p_X(x)$.</p>

<p>If X is a continuous random variable with PDF $f_X(x)$, then the expected value of g(X) is defined as,</p>

<p>$E[g(X)] = \int^{\infty}_{-\infty} g(x)fX(x)dx.$</p>

<p>Intuitively, the expectation of g(X) can be thought of as a “weighted average” of the values that g(x) can taken on for different values of x, where the weights are given by pX(x) or fX(x). As a special case of the above, note that the expectation, E[X] of a random variable itself is found by letting g(x) = x; this is also known as the mean of the random variable X.</p>

<h5 id="properties-4">Properties:</h5>
<ul>
  <li>$E[a] = a$ for any constant $a \in I!R$.</li>
  <li>$E[af(X)] = aE[f(X)]$ for any constant $a \in I!R$.</li>
  <li>(Linearity of Expectation) $E[f(X) + g(X)] = E[f(X)] + E[g(X)]$.</li>
  <li>For a discrete random variable $X$, $E[1{X = k}] = P(X = k)$.</li>
</ul>

<h4 id="25-variance">2.5 Variance</h4>

<p>The variance of a random variable X is a measure of how concentrated the distribution of a random variable X is around its mean. Formally, the variance of a random variable X is defined as $Var[X] = E[(X − E(X))^2]$</p>

<p>Using the properties in the previous section, we can derive an alternate expression for the variance:</p>

<p>$E[(X − E[X])^2] = E[X^2 − 2E[X]X + E[X]^2] = E[X^2] − 2E[X]E[X] + E[X]^2 = E[X^2] − E[X]^2$,</p>

<p>where the second equality follows from linearity of expectations and the fact that $E[X]$ is actually a
constant with respect to the outer expectation.</p>

<h5 id="properties-5">Properties:</h5>
<ul>
  <li>$Var[a] = 0$ for any constant $a \in I!R$.</li>
  <li>$Var[af(X)] = a^2 Var[f(X)]$ for any constant $a \in I!R$.</li>
</ul>

<p><strong>Example</strong> Calculate the mean and the variance of the uniform random variable X with PDF $f_X(x) = 1, ∀x ∈ [0, 1], 0$ elsewhere.</p>

<p>$E[X] = \int^{\infty}_{-\infty} x f_x(x) dx = \int^1_0 x dx = \frac{1}{2}$</p>

<p>$E[X^2] = \int^{\infty}_{-\infty} x^2 f_X(x)dx = \int^1_0 x^2 dx = \frac{1}{3}$</p>

<p>$Var[X] = E[X^2] - E[X]^2 = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}$</p>

<p><strong>Example</strong> : Suppose that $g(x) = 1{x \in A}$ for some subset $A ⊆ \Omega$. What is $E[g(X)]$?</p>

<h5 id="discrete-case">Discrete case:</h5>

<p>$E[g(X)] = \sum_{x \in Val(X)} 1{x \in A}P_X(x)dx = \sum_{x∈A} P_X(x)dx = P(x \in A)$</p>

<h5 id="continuous-case">Continuous case:</h5>

<p>$E[g(X)] = \int^{\infty}<em>{-\infty} 1{x ∈ A}f_X(x)dx = \int</em>{x \in A} f_X(x)dx = P(x \in A)$.</p>

<h4 id="26-some-common-random-variables">2.6 Some common random variables</h4>

<h5 id="discrete-random-variables">Discrete random variables</h5>
<p>• <strong>$X$ ∼ Bernoulli(p)</strong> $(where 0 ≤ p ≤ 1)$: one if a coin with heads probability p comes up
heads, zero otherwise.
\begin{equation}
  p(x)=\begin{cases}
    p, &amp; \text{if $p = 1$}.<br />
    1-p, &amp; \text{if $p = 0$}.
  \end{cases}
\end{equation}
• <strong>$X$ ∼ Binomial(n, p)</strong> $(where 0 ≤ p ≤ 1)$: the number of heads in n independent flips of a
coin with heads probability p.
\begin{equation}
p(x) = \binom{n}{x} \cdot p^x q^{n-x}
\end{equation}
• <strong>$X$ ∼ Geometric(p)</strong> $(where p &gt; 0)$: the number of flips of a coin with heads probability p
until the first heads.
\begin{equation} p(x) = p(1 − p)^{x-1}
\end{equation}
• <strong>$X$ ∼ Poisson(λ)</strong> (where λ &gt; 0): a probability distribution over the nonnegative integers
used for modeling the frequency of rare events.
\begin{equation} 
p(x) = e^{\lambda} \frac{\lambda^x}{x!}
\end{equation}</p>

<h5 id="continuous-random-variables">Continuous random variables</h5>

<p>• <strong>$X$ ∼ Uniform(a, b)</strong> (where $a &lt; b$): equal probability density to every value between a and b on the real line. \begin{equation}
f(x)=\begin{cases}  <br />
\frac{1}{b-a}, &amp; \text{if $a \leq b$}.<br />
0, &amp; \text{otherwise}.
\end{cases}
\end{equation}
• <strong>$X$ ∼ Exponential(λ)</strong> (where λ &gt; 0): decaying probability density over the nonnegative reals.
\begin{equation}
  f(x)=\begin{cases}
    \lambda e^{-\lambda x}, &amp; \text{if $x \geq 0$}.<br />
    0, &amp; \text{otherwise}.
  \end{cases}
\end{equation}</p>

<p>• <strong>$X$ ∼ Normal($\mu$, $\sigma^2$)</strong>: also known as the Gaussian distribution
\begin{equation}
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
\end{equation}
<!--Figure 2: PDF and CDF of a couple of random variables.
The shape of the PDFs and CDFs of some of these random variables are shown in Figure ??.--></p>

<h4 id="31-joint-and-marginal-distributions">3.1 Joint and marginal distributions</h4>

<p>Suppose that we have two random variables X and Y . One way to work with these two random variables is to consider each of them separately. If we do that we will only need $F_X(x)$ and $F_Y (y)$. But if we want to know about the values that X and Y assume simultaneously during outcomes of a random experiment, we require a more complicated structure known as the joint cumulative distribution function of X and Y , defined by 
\begin{equation}F_{XY} (x, y) = P(X ≤ x, Y ≤ y)\end{equation}</p>

<p>It can be shown that by knowing the joint cumulative distribution function, the probability of any event involving X and Y can be calculated.</p>

<p>The joint CDF FXY (x, y) and the joint distribution functions FX(x) and FY (y) of each variable separately are related by
\begin{equation}F_X(x) = lim_{y→\infty} F_{XY} (x, y)dy<br />
F_Y(y) = lim_{x→\infty} F_{XY} (x, y)dx
\end{equation}
Here, we call $F_X(x)$ and $F_Y(y)$ the <strong>marginal cumulative distribution functions</strong> of $F_{XY} (x, y)$.</p>

<h5 id="properties-6">Properties:</h5>
<ul>
  <li>$0 \leq F_{XY} (x, y) \leq 1$.</li>
  <li>$lim_{x,y\rightarrow \infty} F_{XY} (x, y) = 1$.</li>
  <li>$lim_{x,y\rightarrow -\infty} F_{XY} (x, y) = 0$.</li>
  <li>$F_X(x) = lim_{y \rightarrow \infty} F_{XY} (x, y)$.</li>
</ul>

<h4 id="32-joint-and-marginal-probability-mass-functions">3.2 Joint and marginal probability mass functions</h4>

<p>If X and Y are discrete random variables, then the joint probability mass function $p_{XY} : I!R \prod !R →[0, 1]$ is defined by
\begin{equation}
p_{XY}(x, y) = P(X = x, Y = y).
\end{equation}
Here, $0 \leq P_{XY}(x, y) \leq 1$ for all $x, y,$ and 
$\sum_{x \in Val(X)} \sum_{y \in Val(Y)} P_{XY}(x, y) = 1$.</p>

<p>How does the joint PMF over two variables relate to the probability mass function for each variable
separately? It turns out that
\begin{equation}
p_X(x) = \sum_y p_{XY} (x, y).
\end{equation}
and similarly for $p_Y (y)$. In this case, we refer to $p_X(x)$ as the <strong>marginal probability mass function</strong>
of X. In statistics, the process of forming the marginal distribution with respect to one variable by
summing out the other variable is often known as “marginalization.”</p>

<h4 id="33-joint-and-marginal-probability-density-functions">3.3 Joint and marginal probability density functions</h4>

<p>Let X and Y be two continuous random variables with joint distribution function $F_{XY}$ . In the case that $F_{XY}(x, y)$ is everywhere differentiable in both x and y, then we can define the joint probability density function,</p>

<p>\begin{equation}
f_{XY}(x, y) = \frac{∂2F_{XY}(x, y)}{∂x∂y}
\end{equation}</p>

<p>Like in the single-dimensional case, $f_{XY} (x, y) \neq P(X = x, Y = y)$, but rather
\begin{equation}
\int \int_{x∈A} f_{XY} (x, y)dx dy = P((X, Y ) \in A).
\end{equation}
Note that the values of the probability density function f_{XY}(x, y) are always nonnegative, but they
may be greater than 1. Nonetheless, it must be the case that $\int^{\infty}<em>{-\infty} \int^{\infty}</em>{-\infty} f_{XY}(x,y) = 1$</p>

<p>Analagous to the discrete case, we define
\begin{equation}
f_X(x) = \int^{\infty}<em>{-\infty} f</em>{XY} (x, y)dy,
\end{equation}
as the <strong>marginal probability density function</strong> (or <strong>marginal density</strong>) of X, and similarly for $f_Y (y)$.</p>

<h4 id="34-conditional-distributions">3.4 Conditional distributions</h4>

<p>Conditional distributions seek to answer the question, what is the probability distribution over Y, when we know that X must take on a certain value x? In the discrete case, the conditional probability mass function of X given Y is simply 
\begin{equation}
p_{Y \mid X} (y \mid x) = \frac{p_{XY}(x, y)}{p_X(x)},
\end{equation}
assuming that p_X(x) \neq 0.</p>

<p>In the continuous case, the situation is technically a little more complicated because the probability that a continuous random variable X takes on a specific value x is equal to zero. Ignoring this
technical point, we simply define, by analogy to the discrete case, the <em>conditional probability density</em> of Y given X = x to be
\begin{equation}
f_{Y \mid X}(y \mid x) = \frac{f_{XY} (x, y)}{f_X(x)}
\end{equation}
provided $f_X(x) \neq 0$.</p>

<h4 id="35-bayess-rule">3.5 Bayes’s rule</h4>

<p>A useful formula that often arises when trying to derive expression for the conditional probability of
one variable given another, is <strong>Bayes’s rule</strong>.</p>

<p>In the case of discrete random variables X and Y ,
\begin{equation}
P_{Y \mid X}(y|x) = \frac{P_{XY}(x, y)}{P_X(x)} = \frac{P_{X \mid Y} (x \mid y) P_Y(y)}{\sum_{y’ \in Val(Y) P_{X \mid Y} (x \mid y’) P_Y(y’)}}
\end{equation}</p>

<p>If the random variables X and Y are continuous,
\begin{equation}
f_{Y \mid X}(y|x) = \frac{f_{XY}(x, y)}{f_X(x)} = \frac{f_{X \mid Y} (x \mid y) f_Y(y)}{\int^{\infty}<em>{- \infty} f</em>{X\mid Y} (x \mid y’) f_Y (y’) dy’}
\end{equation}</p>

<h4 id="36-independence">3.6 Independence</h4>
<p>Two random variables X and Y are independent if $F_{XY} (x, y) = F_X(x)F_Y(y)$ for all values of x and y. Equivalently,</p>
<ul>
  <li>For discrete random variables, $p_{XY} (x, y) = p_X(x)p_Y(y)$ for all $x \in Val(X)$, $y \in Val(Y)$.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>For discrete random variables, $pY</td>
          <td>X(y</td>
          <td>x) = pY (y)$ whenever $p_X(x) \neq 0$ for all $y \in Val(Y)$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>For continuous random variables, $f_{XY} (x, y) = f_X(x)f_Y(y)$ for all $x, y \in R$.</li>
  <li>For continuous random variables, $f_{Y\mid X}(y \mid x) = f_Y(y)$ whenever $f_X(x) \neq 0$ for all $y \in R$.</li>
</ul>

<p>Informally, two random variables $X$ and $Y$ are independent if “knowing” the value of one variable will never have any effect on the conditional probability distribution of the other variable, that is, you know all the information about the pair $(X, Y)$ by just knowing $f(x)$ and $f(y)$. The following lemma formalizes this observation:</p>

<p><strong>Lemma 3.1.</strong> If $X$ and $Y$ are independent then for any subsets $A, B ⊆ I!R$, we have,
\begin{equation}
P(X \in A, Y \in B) = P(X \in A)P(Y \in B)
\end{equation}
By using the above lemma one can prove that if $X$ is independent of $Y$ then any function of X is independent of any function of Y.</p>

<h4 id="37-expectation-and-covariance">3.7 Expectation and covariance</h4>

<p>Suppose that we have two discrete random variables $X, Y$ and $g : {I!R}^2 \rightarrow I!R$ is a function of these two random variables. Then the expected value of g is defined in the following way, 
\begin{equation}
E[g(X,Y)] = \sum_{x \in Val(X)} \sum_{y \in Val(Y)} g(x, y)p_{XY}(x, y).
\end{equation}</p>

<p>For continuous random variables X, Y , the analogous expression is
\begin{equation}
E[g(X, Y)] = \int^{\infty}<em>{-\infty} \int^{\infty}</em>{-\infty} g(x, y)f_{XY}(x, y)dxdy.
\end{equation}</p>

<p>We can use the concept of expectation to study the relationship of two random variables with each other. In particular, the covariance of two random variables X and Y is defined as
\begin {equation}
Cov[X, Y] = E[(X − E[X])(Y − E[Y])]
\end{equation}</p>

<p>Using an argument similar to that for variance, we can rewrite this as,
\begin{equation}
Cov[X, Y] = E[(X − E[X])(Y − E[Y])] <br />
= E[XY − XE[Y] − Y E[X] + E[X]E[Y]] <br />
= E[XY] − E[X]E[Y] − E[Y]E[X] + E[X]E[Y]] <br />
= E[XY] − E[X]E[Y].
\end{equation}</p>

<p>Here, the key step in showing the equality of the two forms of covariance is in the third equality, where we use the fact that $E[X]$ and $E[Y]$ are actually constants which can be pulled out of the expectation. When $Cov[X, Y] = 0$, we say that $X$ and $Y$ are uncorrelated.</p>

<h5 id="properties-7">Properties:</h5>
<ul>
  <li>(Linearity of expectation) $E[f(X, Y) + g(X, Y)] = E[f(X, Y)] + E[g(X, Y)]$.</li>
  <li>$Var[X + Y] = Var[X] + Var[Y] + 2Cov[X, Y]$.</li>
  <li>If $X$ and $Y$ are independent, then $Cov[X, Y] = 0$.</li>
  <li>If $X$ and $Y$ are independent, then $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$.</li>
</ul>




    </article>
    <span class="print-footer">Probability Review - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2017 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2017</span> 
</div>  
</footer>

  </body>
</html>
